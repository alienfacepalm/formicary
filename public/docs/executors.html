<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="/favicon.ico">

    <title>Formicary Distributed Job Management Dashboard</title>

    <link rel="canonical" href="/dashboard">

    <!-- Bootstrap core CSS -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="/css/custom.css" rel="stylesheet">
    <link href="/css/style.default.css" rel="stylesheet" id="theme-stylesheet">

    <!-- Font Awesome -->
    <link
            href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css"
            rel="stylesheet"
    />
    <!-- MDB
    <link
            href="https://cdnjs.cloudflare.com/ajax/libs/mdb-ui-kit/3.2.0/mdb.min.css"
            rel="stylesheet"
    />
    -->
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="https://code.jquery.com/jquery-3.5.1.slim.js"><\/script>')</script>
    <script src="https://pagecdn.io/lib/popper/2.4.4/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

    <!-- Icons -->
    <script src="https://unpkg.com/feather-icons/dist/feather.min.js"></script>
    <script>
      feather.replace()
    </script>
</head>

<body>

<div class="container-fluid">
<h2 id="executors">Executors</h2>
<h3 id="shell-or-local-executor">Shell or Local Executor</h3>
<p>The shell executor forks a shell process from ant work for executing commands defined under <code>script</code>. It does not require any additional configuration, but it&#39;s recommended to use a unique user for the ant worker with proper permissions because a user may invoke any command on the machine.</p>
<h3 id="rest">REST</h3>
<p>RES API Executor invokes external HTTP APIs using GET, POST, PUT or DELTE actions, e.g.</p>
<pre><code>job_type: http-job
tasks:
- task_type: get
  method: HTTP_GET
  script:
    - https://jsonplaceholder.typicode.com/todos/1
  on_completed: post
- task_type: post
  method: HTTP_POST_JSON
  script:
    - https://jsonplaceholder.typicode.com/todos
  on_completed: put
- task_type: put
  method: HTTP_PUT_JSON
  script:
    - https://jsonplaceholder.typicode.com/todos/1
  on_completed: delete
- task_type: delete
  method: HTTP_DELETE
  script:
    - https://jsonplaceholder.typicode.com/todos/1
</code></pre><h3 id="docker">Docker</h3>
<p>The Docker executor starts a main container for executing script named after job/task name and a helper container wth <code>-helper</code> suffix for managing artifacts. The initial docker config are defined by the ant config that are available for all jobs such as:</p>
<ul>
<li>helper_image - helper image</li>
<li>host - Docker host</li>
<li>registry server - docker registry</li>
<li>environment - environment variables</li>
<li>pull_policy - image pull policy such as <code>never</code>, <code>always</code>, <code>if-not-present</code>.</li>
</ul>
<pre><code class="lang-yaml">common:
  id: test-id
  messaging_provider: &quot;REDIS_MESSAGING&quot;
tags:
  - tag1
  - tag2
methods:
  - DOCKER
docker:
  registry:
    registry: docker-registry-server
    username: docker-registry-user
    password: docker-registry-pass
    pull_policy: if-not-present
  host: kubernetes-host
</code></pre>
<p>Above configuration applies to all jobs, but a docker task can define following properties for each job-definition:</p>
<ul>
<li>name - the name of task that is used for pod-name</li>
<li>environment - environment variables to set within the container</li>
<li>working_directory - for script execution</li>
<li>container - main container to execute, which defines:<ul>
<li>image</li>
<li>image_definition</li>
</ul>
</li>
<li>network_mode</li>
<li>host_network e.g.,</li>
</ul>
<pre><code class="lang-yaml">name: task1
method: DOCKER
environment:
  AWS-KEY: Mykey
container:
  image: ubuntu:16.04
privileged: true
network_mode: mod1
host_network: true
</code></pre>
<h3 id="kubernetes">Kubernetes</h3>
<p>The Kubernetes executor starts a main container for executing script named after job/task name and a helper container wth <code>-helper</code> suffix for managing artifacts. A task may define dependent services that will start with <code>svc-</code> prefix. The initial kubernetes config are defined by the ant config that are available for all jobs such as:</p>
<ul>
<li>namespace - namespace of Kubernetes environment</li>
<li>helper_image - helper image</li>
<li>bearer_token - bearer token for launching pods</li>
<li>host - Kubernetes api server (optional)</li>
<li>cert_file - api server cert</li>
<li>key_file - api server key</li>
<li>ca_file - api server ca</li>
<li>service_account - array of accounts to use for pods</li>
<li>image_pull_secrets - array of secrets for pulling docker images</li>
<li>dns_policy such as <code>none</code>, <code>default</code>, <code>cluster-first</code>, <code>cluster-first-with-host-net</code>.</li>
<li>dns_config such as <code>nameservers</code>, <code>options</code>, <code>searches</code></li>
<li>volumes - to mount on pods</li>
<li>pod_security_context</li>
<li>host_aliases - array of host aliases</li>
<li>cap_add - array of linux capabilities to add for pods</li>
<li>cap_drop - array of linux capabilities to drop for pods</li>
<li>environment - environment variables</li>
<li>pull_policy - image pull policy such as <code>never</code>, <code>always</code>, <code>if-not-present</code>.</li>
</ul>
<pre><code class="lang-yaml">common:
  id: test-id
  messaging_provider: &quot;REDIS_MESSAGING&quot;
tags:
  - tag1
  - tag2
methods:
  - KUBERNETES
kubernetes:
  registry:
    registry: docker-registry-server
    username: docker-registry-user
    password: docker-registry-pass
    pull_policy: if-not-present
  host: kubernetes-host
  bearer_token: kubernetes-bearer
  cert_file: kubernetes-cert
  key_file: kubernetes-key
  ca_file: kubernetes-cafile
  namespace: default
  service_account: my-svc-account
  image_pull_secrets:
    - image-pull-secret
  dns_policy: none
  pod_security_context:
    fs_group: 100
    run_as_group: 100
    run_as_non_root: true
    run_as_user: 1000
    supplemental_groups:
      - 200
      - 300
  cap_add:
    - NET_RAW
    - CAP1
  cap_drop:
    - CAP2
</code></pre>
<p>Above configuration applies to all jobs, but a kubernetes task can define following properties for each job-definition:</p>
<ul>
<li>name - the name of task that is used for pod-name</li>
<li>environment - environment variables to set within the container</li>
<li>working_directory - for script execution</li>
<li>container - main container to execute, which defines:<ul>
<li>image</li>
<li>image_definition</li>
<li>volumes based on host, pvc, config_map, secret and empty<ul>
<li>host mounts folder from the host path</li>
<li>pvc uses persistent volume claim defined in the kubernetes cluster</li>
<li>config_map uses config map defined in the kubernetes cluster, it defines <code>items</code> to add keys and relative path</li>
<li>secret mounts secret as a volume, it defines <code>items</code> to add keys and relative path</li>
<li>empty mounts an empty volume</li>
</ul>
</li>
<li>volume_driver</li>
<li>devices - array of devices</li>
<li>bind_directory</li>
<li>cpu_limit - cpu allocation given</li>
<li>cpu_request - cpu allocation requested</li>
<li>memory_limit - memory allocated</li>
<li>memory_request - memory requested</li>
</ul>
</li>
<li>services - array of services<ul>
<li>name - service name</li>
<li>image - service image</li>
<li>command - service command</li>
<li>entrypoint - service entrypoint</li>
<li>volumes - volumes</li>
<li>cpu_limit - cpu allocation given</li>
<li>cpu_request - cpu allocation requested</li>
<li>memory_limit - memory allocated</li>
<li>memory_request - memory requested</li>
</ul>
</li>
<li>affinity - affinity for specifying nodes to use for execution</li>
<li>node_selector - key/value pairs for selecting node with matching tolerated tainted nodes</li>
<li>node_tolerations</li>
<li>pod_label - key/value pairs</li>
<li>pod_annotations - key/value pairs</li>
<li>network_mode</li>
<li>host_network e.g.,</li>
</ul>
<pre><code class="lang-yaml">name: task1
method: KUBERNETES
environment:
  AWS-KEY: Mykey
container:
  image: ubuntu:16.04
  volumes:
    host_path:
      - name: mount1
        mount_path: /shared
        host_path: /host/shared
    pvc:
      - name: mount2
        mount_path: /mnt/sh1
    config_map:
      - name: mount3
        mount_path: /mnt/sh2
        items:
          item1: val1
    secret:
      - name: mount4
        mount_path: /mnt/sh3
        items:
          item1: val1
    empty_dir:
      - name: mount4
        mount_path: /mnt/sh3
  volume_driver: voldriver
  devices:
    - devices
  bind_dir: /shared
  cpu_limit: &quot;1&quot;
  cpu_request: 500m
  memory_limit: 1Gi
  memory_request: 1Gi
services:
  - name: svc-name
    image: ubuntu:16.04
    command:
      - .html1
    entrypoint:
      - /bin/bash
    volumes:
      host_path:
        - name: svc-mount1
          mount_path: /shared
          host_path: /host/shared
          read_only: false
      pvc:
        - name: svc-mount2
          mount_path: /mnt/sh1
          read_only: true
      config_map:
        - name: svc-mount3
          mount_path: /mnt/sh2
          read_only: true
          items:
            item1: val1
      secret:
        - name: svc-mount4
          mount_path: /mnt/sh3
          items:
            mysecret: file-name
      empty_dir:
        - name: svc-mount5
          mount_path: /mnt/sh3
    cpu_limit: &quot;1&quot;
    cpu_request: 500m
    memory_limit: 1Gi
    memory_request: 1Gi
privileged: true
affinity:
  required_during_scheduling_ignored_during_execution:
    node_selector_terms:
      - match_expressions:
          - key: datacenter
            operator: In
            values:
              - seattle
        match_fields:
          - key: key2
            operator: In
            values:
              - val2
  preferred_during_scheduling_ignored_during_execution:
    - weight: 1
      preference:
        match_expressions:
          - key: datacenter
            operator: In
            values:
              - chicago
        match_fields:
          - key: color
            operator: In
            values:
              - blue
node_selector:
  formicary: &quot;true&quot;
node_tolerations:
  empty: PreferNoSchedule
  myrole: NoSchedule
pod_labels:
  foo: bar
pod_annotations:
  ann1: val
network_mode: mod1
host_network: true
</code></pre>
<h3 id="customized">Customized</h3>
<p>You can implement a customized executor by subscribing to the messaging queue, e.g. here is a sample messaging executor:</p>
<pre><code class="lang-go">// MessagingHandler structure
type MessagingHandler struct {
    id            string
    requestTopic  string
    responseTopic string
    queueClient   queue.Client
}

// NewMessagingHandler constructor
func NewMessagingHandler(
    id string,
    requestTopic string,
    responseTopic string,
    queueClient queue.Client,
) *MessagingHandler {
    return &amp;MessagingHandler{
        id:            id,
        requestTopic:  requestTopic,
        responseTopic: responseTopic,
        queueClient:   queueClient,
    }
}

func (h *MessagingHandler) Start(
    ctx context.Context,
) (err error) {
    return h.queueClient.Subscribe(
        ctx,
        h.requestTopic,
        h.id,
        make(map[string]string),
        true, // shared subscription
        func(ctx context.Context, event *queue.MessageEvent) error {
            defer event.Ack()
            err = h.execute(ctx, event.Payload)
            if err != nil {
                logrus.WithFields(logrus.Fields{
                    &quot;Component&quot;: &quot;MessagingHandler&quot;,
                    &quot;Payload&quot;:   string(event.Payload),
                    &quot;Target&quot;:    h.id,
                    &quot;Error&quot;:     err}).Error(&quot;failed to execute&quot;)
                return err
            }
            return nil
        },
    )
}

// Stop stops subscription
func (h *MessagingHandler) Stop(
    ctx context.Context,
) (err error) {
    return h.queueClient.UnSubscribe(
        ctx,
        h.requestTopic,
        h.id,
    )
}

// execute incoming request
func (h *MessagingHandler) execute(
    ctx context.Context,
    reqPayload []byte) (err error) {
    var req types.TaskRequest
    err = json.Unmarshal(reqPayload, &amp;req)
    if err != nil {
        return err
    }
    resp := types.NewTaskResponse(&amp;req)

    // Implement business logic below
    epoch := time.Now().Unix()
    if epoch%2 == 0 {
        resp.Status = types.COMPLETED
    } else {
        resp.ErrorCode = &quot;ERR_MESSAGING_WORKER&quot;
        resp.ErrorMessage = &quot;mock error for messaging client&quot;
        resp.Status = types.FAILED
    }
    resp.AddContext(&quot;epoch&quot;, epoch)

    // Send back reply
    resPayload, err := json.Marshal(resp)
    if err != nil {
        return err
    }
    _, err = h.queueClient.Send(
        ctx,
        h.responseTopic,
        make(map[string]string),
        resPayload,
        false)
    return err
}
</code></pre>
<p>Here is an equivalent implementation of messaging ant worker in Javascript:</p>
<pre><code class="lang-javascript">const {Kafka} = require(&#39;kafkajs&#39;)
const assert = require(&#39;assert&#39;)

// Messaging helper class for communication with the queen server
class Messaging {
    constructor(config) {
        assert(config.clientId, `clientId is not specified`)
        assert(config.brokers.length, &#39;brokers is not specified&#39;)
        config.reconnectTimeout = config.reconnectTimeout || 10000
        this.config = config
        this.kafka = new Kafka(config)
        this.producers = {}
        console.info({config}, `initializing messaging helper`)
    }

    // send a message to the topic
    async send(topic, headers, message) {
        const producer = await this.getProducer(topic)
        await producer.send({
            topic: topic,
            messages: [
                {
                    key: headers[&#39;key&#39;],
                    headers: headers,
                    value: typeof message == &#39;object&#39; ? JSON.stringify(message) : message,
                }
            ]
        })
        console.debug({topic, message}, `sent message to ${topic}`)
    }

    // subscribe to topic with given callback
    async subscribe(topic, cb, tries = 0) {
        //const meta = await this.getTopicMetadata(topic)
        const consumer = this.kafka.consumer({groupId: this.config.groupId})
        try {
            await consumer.connect()
            const subConfig = {topic: topic, fromBeginning: false}
            await consumer.subscribe(subConfig)
            const handle = (message, topic, partition) =&gt; {
                const headers = message.headers || {}
                if (topic) {
                    headers.topic = topic
                }
                if (partition) {
                    headers.partition = partition.toString()
                }

                headers.key = (message.key || &#39;&#39;).toString()
                headers.timestamp = message.timestamp
                headers.size = (message.size || &#39;0&#39;).toString()
                headers.attributes = (message.attributes || &#39;[]&#39;).toString()
                headers.offset = message.offset
                cb(headers, JSON.parse(message.value || &#39;&#39;))
            }

            console.info({topic, groupId: this.config.groupId},
                `subscribing consumer to ${topic}`)
            await consumer.run({
                eachBatchAutoResolve: false,
                eachBatch: async ({batch, resolveOffset, heartbeat, isRunning, isStale}) =&gt; {
                    for (let message of batch.messages) {
                        if (!isRunning()) break
                        if (isStale()) continue
                        handle(message, topic)
                        resolveOffset(message.offset) // commit
                        await heartbeat()
                    }
                }
            })

            return () =&gt; {
                console.info({topic, groupId: this.config.groupId}, `closing consumer`)
                consumer.disconnect()
            }
        } catch (e) {
            console.warn(
                {topic, error: e, config: this.config},
                `could not subscribe, will try again`)
            tries++
            return new Promise((resolve) =&gt; {
                setTimeout(() =&gt; {
                    resolve(this.subscribe(topic, cb, tries))
                }, Math.min(tries * 1000, this.config.reconnectTimeout + 5000))
            })
        }
    }

    // getTopicMetadata returns partition metadata for the topic
    async getTopicMetadata(topic, groupId, tries = 0) {
        const admin = this.kafka.admin()
        try {
            await admin.connect()
            const response = {}
            if (groupId) {
                response.offset = await admin.fetchOffsets({groupId, topic})
            } else {
                response.offset = await admin.fetchTopicOffsets(topic)
            }
            const meta = await admin.fetchTopicMetadata({topics: [topic]})
            if (meta &amp;&amp; meta.topics &amp;&amp; meta.topics[0].partitions) {
                response.partitions = meta.topics[0].partitions
            }
            response.topics = await admin.listTopics()
            response.groups = (await admin.listGroups())[&#39;groups&#39;]
            return response
        } catch (e) {
            console.warn(
                {topic, error: e, config: this.config},
                `could not get metadata, will try again`)
            tries++
            return new Promise((resolve) =&gt; {
                setTimeout(() =&gt; {
                    resolve(this.getTopicMetadata(topic, groupId, tries))
                }, Math.min(tries * 1000, this.config.reconnectTimeout))
            })
        }
    }

    // getProducer returns producer for the topic
    async getProducer(topic, tries = 0) {
        const producer = this.kafka.producer()
        try {
            await producer.connect()
            this.producers[topic] = producer
            console.info({topic, tries}, `adding producer`)
            return producer
        } catch (e) {
            console.warn(
                {topic, error: e, config: this.config},
                `could not get messaging producer, will try again`)
            tries++
            return new Promise((resolve) =&gt; {
                setTimeout(() =&gt; {
                    resolve(this.getProducer(topic, tries))
                }, Math.min(tries * 1000, this.config.reconnectTimeout))
            })
        }
    }

    // closeProducers closes producers
    async closeProducers() {
        Object.entries(this.producers).forEach(([topic, producer]) =&gt; {
            console.info({topic}, `closing producer`)
            try {
                producer.disconnect()
            } catch (e) {
            }
        })
        this.producers = {}
    }

}


const Messaging = require(&#39;./messaging&#39;)
const process = require(&quot;process&quot;)
const readline = require(&quot;readline&quot;)

const conf = {
    clientId: &#39;messaging-js-client&#39;,
    groupId: &#39;messaging-js-client&#39;,
    brokers: [&#39;127.0.0.1:19092&#39;, &#39;127.0.0.1:29092&#39;, &#39;127.0.0.1:39092&#39;],
    inTopic: &#39;formicary-message-ant-request&#39;,
    outTopic: &#39;formicary-message-ant-response&#39;,
    connectionTimeout: 10000,
    requestTimeout: 10000,
}

const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout
})

rl.on(&quot;close&quot;, () =&gt; {
    process.exit(0)
});

const messaging = new Messaging(conf)
messaging.subscribe(conf.inTopic, async (headers, msg) =&gt; {
    console.log({headers, msg}, `received from ${conf.inTopic}`)
    msg.status = &#39;COMPLETED&#39;
    msg.taskContext = {&#39;key1&#39;: &#39;value1&#39;}
    messaging.send(conf.outTopic, headers, msg)
}).then(() =&gt; {
    rl.question(&#39;press enter to exit&#39;, () =&gt; {
        rl.close()
    })
})
</code></pre>
<p>Here is a sample job definition that uses <code>MESSAGING</code> executor:</p>
<pre><code class="lang-yaml">job_type: messaging-job
timeout: 60s
tasks:
- task_type: trigger
  method: MESSAGING
  messaging_request_queue: formicary-message-ant-request
  messaging_reply_queue: formicary-message-ant-response
</code></pre>
<h3 id="task-request">Task Request</h3>
<p>The task request is sent to the ant work for executing the work and includes following properties:</p>
<pre><code>    {
        &quot;user_id&quot;: &quot;uuid&quot;,
        &quot;organization_id&quot;: &quot;uuid&quot;,
        &quot;job_definition_id&quot;: &quot;uuid&quot;,
        &quot;job_request_id&quot;: 1,
        &quot;job_type&quot;: &quot;my-test-job&quot;,
        &quot;task_type&quot;: &quot;task1&quot;,
        &quot;job_execution_id&quot;: &quot;uuid&quot;,
        &quot;task_execution_id&quot;: &quot;uuid&quot;,
        &quot;task_id&quot;: &quot;uuid&quot;,
        &quot;co_relation_id&quot;: &quot;uuid&quot;,
        &quot;tags&quot;: [],
        &quot;platform&quot;: &quot;linux&quot;,
        &quot;action&quot;: &quot;EXECUTE&quot;,
        &quot;job_retry&quot;: 0,
        &quot;task_retry&quot;: 0,
        &quot;allow_failure&quot;: false,
        &quot;before_script&quot;: [&quot;t1_.html1&quot;, &quot;t1_.html2&quot;, &quot;t1_.html3&quot;],
        &quot;after_script&quot;: [&quot;t1_.html1&quot;, &quot;t1_.html2&quot;, &quot;t1_.html3&quot;],
        &quot;script&quot;: [&quot;t1_.html1&quot;, &quot;t1_.html2&quot;, &quot;t1_.html3&quot;],
        &quot;timeout&quot;: 0,
        &quot;variables&quot;: {
            &quot;jk1&quot;: {&quot;name&quot;: &quot;&quot;, &quot;value&quot;: &quot;jv1&quot;, &quot;secret&quot;: false},
            &quot;jk2&quot;: {&quot;name&quot;: &quot;&quot;, &quot;value&quot;: {&quot;a&quot;: 1, &quot;b&quot;: 2}, &quot;secret&quot;: false},
        },
        &quot;executor_opts&quot;: {
            &quot;name&quot;: &quot;frm-1-task1-0-0-6966&quot;,
            &quot;method&quot;: &quot;KUBERNETES&quot;,
            &quot;container&quot;: {&quot;image&quot;: &quot;&quot;, &quot;imageDefinition&quot;: {}},
            &quot;helper&quot;: {&quot;image&quot;: &quot;&quot;, &quot;imageDefinition&quot;: {}},
            &quot;headers&quot;: {&quot;t1_h1&quot;: &quot;1&quot;, &quot;t1_h2&quot;: &quot;true&quot;, &quot;t1_h3&quot;: &quot;three&quot;}
        },
    }

</code></pre><h3 id="task-response">Task Response</h3>
<p>The task response is sent back by the ant work after executing the work and includes following properties:</p>
<pre><code>    {
        &quot;job_request_id&quot;: 1,
        &quot;job_type&quot;: &quot;my-test-job&quot;,
        &quot;task_type&quot;: &quot;task1&quot;,
        &quot;task_id&quot;: &quot;uuid&quot;,
        &quot;co_relation_id&quot;: &quot;uuid&quot;,
        &quot;status&quot;: &quot;COMPLETED&quot;,
        &quot;co_relation_id&quot;: &quot;uuid&quot;,
        &quot;ant_id&quot;: &quot;&quot;,
        &quot;host&quot;: &quot;&quot;,
        &quot;namespace&quot;: &quot;&quot;,
        &quot;tags&quot;: [],
        &quot;error_message&quot;: &quot;&quot;,
        &quot;error_code&quot;: &quot;&quot;,
        &quot;exit_code&quot;: &quot;&quot;,
        &quot;exit_message&quot;: &quot;&quot;,
        &quot;failed_command&quot;: &quot;&quot;,
        &quot;task_context&quot;: {},
        &quot;job_context&quot;: {},
        &quot;warnings&quot;: [],
        &quot;cost_factor&quot;: 0,
    }

</code></pre></div>  
</body>
</html>
